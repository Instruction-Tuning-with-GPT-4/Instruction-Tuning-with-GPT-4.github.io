<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    <script src="https://cdn.staticfile.org/jquery/2.1.1/jquery.min.js"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>

    <!-- <link rel="stylesheet" href="https://themesguide.github.io/top-hat/dist/minco/theme.min.css"> -->

    

    <title>Instruction Tuning with GPT-4</title>
  </head>
  <style>
    body {padding-top: 100px;}
    /* This text is in Gill Sans */
    .box{ 
      width: 12%; 
      /* height: 100px;  */
      } 
    .img-overlay {
            position: absolute;
            top: 0;
            bottom: 0;
            left: 0;
            right: 0;
            text-align: left;
        }
        .citation {
            display: block;
            padding: 9.5px;
            margin: 0 0 10px;
            font-size: 13px;
            line-height: 1.42857143;
            word-break: break-all;
            word-wrap: break-word;
            color: #333;
            background-color: #f5f5f5;
            border: 1px solid #ccc;
            border-radius: 4px;
        }
.class { 
	font-family: Calibri,Candara,Segoe,Segoe UI,Optima,Arial,sans-serif; 
}
.myhead{
  background-color:#e9ecef;
  border-radius: .3rem;
  margin-bottom: 1rem;
  padding: 2rem 2rem;
  margin-top: -1rem;
}

li {
  margin-bottom: 10px; /* increase bottom margin of <li> elements by 10px */
}
    </style>
  <body>
    
      <nav class="navbar fixed-top navbar-expand-lg navbar-dark" id="navbar1" style="background-color: #242582;">
        <div class="container">
            <a class="navbar-brand mr-1 mb-1 mt-0" href="../">GPT-4-LLM</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#collapsingNavbar">
                <span class="navbar-toggler-icon"></span>
            </button>
            
            <ul class="navbar-nav ml-auto">
                    <li class="nav-item">
                        <a class="nav-link" href="#github">Contact Us</a>
                    </li>                    
                </ul>
        </div>
    </nav>
    <main>
    <div class="container">
      <div class="myhead">
        <h2 style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif">Instruction Tuning with GPT-4</h2>
        <a class="btn btn-primary" href="" role="button" >Paper »</a>        
        <a class="btn btn-info" href="https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM" role="button">Github »</a>
        <!-- <a class="btn btn-secondary" href="#" role="button">Demo (Soon) »</a> -->

        
      </div>

      <div style="text-align: center;">
        <img src="./images/gpt4llama_logo.png" alt="a beautiful painting of a llama following the instructions of the AI robot, by studio ghibli, octane render, brilliantly coloured" width="300" height="300">
      </div>
      <p style="color:gray; text-align: center;">GPT-4-LLM 
        (<em>generated by <a href="https://gligen.github.io/">GLIGEN</a></em>)</p>
      

      <!-- <div class="col-md-12">
        <img class='img-fluid' src="{{url_for('static', filename='model.png')}}">
      </div> -->
      <div class="col-md-12" style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif">
        <!-- <h3 class=".display-3">Pre-training-fine-tuning paradigm for building task-oritend dialog systems </h3> -->
        <p>Large Language Models (LLMs) have shown impressive generalization capabilities such as in-
          context-learning and chain-of-thoughts reasoning. To enable LLMs to follow natural language instructions and complete real-world tasks, researchers have been exploring methods of instruction-tuning of LLMs. To advance the state of the art of instruction-tuning for LLMs, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. 
	
	<h3 class=".display-3">GPT-4 Data</h3>	
	   In particular, we release the following data assets:

          <!-- <ul>
            <li>English Instruction-Following Data: For the 52K instructions collected in Alpaca, one English GPT-4 answer is provided for each.  We leave it as future work to follow an iterative process to construct our own instruction set using GPT-4 and self-instruct.</li>
            <li>Chinese Instruction-Following Data: We use ChatGPT to translate the 52K instructions into Chinese and ask GPT-4 to answer them in Chinese. This allows us to build a Chinese instruction-following model based on LLaMA, and study cross-language generalization ability of instruction-tuning. </li>
            <li>Comparison Data: We ask GPT-4 to rate its own response from 1 to 10. Furthermore, we ask GPT-4 to compare and rate the responses from the three models, including GPT-4, GPT-3.5 and OPT-IML. This is used to train reward models.</li>
            <li>Answers on Unnatural Instructions: The GPT-4 answers are decoded on the core dataset of 68K instruction-input-output triplets. The subset is used to quantify the gap between GPT-4 and our instruction-tuned models at scale.</li>
          </ul> -->

          <ul>
            <li><b>English Instruction-Following Data:</b> for the 52K instructions collected in Alpaca, one English GPT-4 answer is provided for each. </li>
            <li><b>Chinese Instruction-Following Data:</b> we use ChatGPT to translate the 52K instructions into Chinese and ask GPT-4 to answer them in Chinese. </li>
            <li><b>Comparison Data:</b> we ask GPT-4 to rate its own response from 1 to 10. Furthermore, we ask GPT-4 to compare and rate the responses from the three models, including GPT-4, GPT-3.5 and OPT-IML. This is used to train reward models.</li>
            <li><b>Answers on Unnatural Instructions:</b> the GPT-4 answers are decoded on the core dataset of 68K instruction-input-output triplets. The subset is used to quantify the gap between GPT-4 and our instruction-tuned models at scale.</li>
          </ul>
        
        </p>
        <!-- <img class='img-fluid' src="{{url_for('static', filename='input.png')}}"> -->
        
        <div style="padding-left: 13%;">

        </div>

        <h3 class=".display-3">How Good is The Data?</h3>
        
        Evaluating the performance of self-instruct tuned models on GPT-4 data for tasks that have not been seen before remains a difficult task. Our objective is to assess their capability to comprehend and follow instructions for various tasks. To accomplish this, we utilize the following three types of evaluations.Our empirical investigation confirms that the utilization of GPT-4-generated data is an efficient and effective approach for LLM instruction-tuning than other machine generated data.

        
        <ul class="nav nav-tabs" id="myTab" role="tablist">
          <li class="nav-item" style="margin-left: 2px;">
            <a class="nav-link active" data-toggle="tab" href="#human" role="tab" aria-controls="human" style="background-color:azure;">Human Evaluations</a>
          </li>
          <li class="nav-item" style="margin-left: 10px;">
            <a class="nav-link" data-toggle="tab" href="#gpt4" role="tab" aria-controls="gpt4" style="background-color:azure;">GPT-4 Evaluations</a>
          </li>
          <li class="nav-item" style="margin-left: 10px;">
            <a class="nav-link" data-toggle="tab" href="#unnatural" role="tab" aria-controls="unnatural" style="background-color:azure;">Unnatural Instruction Evaluations</a>
          </li>
        </ul>
        
        <div class="tab-content">
          <div class="tab-pane active" id="human" role="tabpanel">
            
            <br>
            Human evaluation was performed on model generation results using Amazon Mechanical Turk following Helpfulness, Honestness and Harmlessness criteria. The results are summarized as follows:
          <ul>
            <li>Two instruction-tuned LLaMA models were compared, fine-tuned on data generated by GPT-4 and GPT-3 respectively.</li>
            <li>LLaMA-GPT-4 performs substantially better than LLaMA-GPT-3 in the "Helpfulness" criterion.</li>
            <li>LLaMA-GPT-4 performs similarly to the original GPT-4 in all three criteria, suggesting a promising direction for developing state-of-the-art instruction-following LLMs.</li>
        </ul>
            
            <div style="text-align: center;">
            <img src="./images/pie_llama_gpt3_vs_llam_gpt4.png" alt="" width="60%">
          </div>
          <p style="color:gray; text-align: center;">
            LLaMA-GPT4 vs Alpaca (i.e., LLaMA-GPT3)
          </p>

          <div style="text-align: center;">
            <img src="./images/pie_llama_gpt4_vs_gpt4.png" alt="" width="60%">
          </div>


          <p style="color:gray; text-align: center;">
            LLaMA-GPT4 vs GPT-4
          </p>
          

            
            
            </div>
          <div class="tab-pane" id="gpt4" role="tabpanel">
            <br>

            GPT-4 was used to evaluate the quality of responses generated by different chatbot models on 80 unseen questions. The responses from LLaMA-GPT-4 (7B) and GPT-4 were collected, and the release answers from other models were obtained from a previous study. GPT-4 was asked to rate the quality of responses between two models using a scale of 1 to 10, and the results were compared against a strong competing model (ChatGPT and GPT-4). <br><br>

            <ul>
              <li>The evaluation showed that the feedback data and reward model were effective in improving the performance of LLaMA.</li>
              <li>LLaMA-GPT-4 outperformed LLaMA and Alpaca, but there was still a performance gap with large commercial chatbots like GPT-4.</li>
              <li>The findings demonstrate the potential of instruction-tuning to enhance the performance of AI chatbots.</li>
              </ul>
            
            <div style="text-align: center;">
              <img src="./images/GPT-4-Evaluations.png" alt="" width="60%">
            </div>

                 <p style="color:gray; text-align: center;">
           Evaluations Scores from GPT-4
          </p>

          

          </div>
          <div class="tab-pane" id="unnatural" role="tabpanel">
            <br>

            <ul>
              <li>Alpaca outperforms LLaMA-GPT4 and GPT-4 in terms of average ROUGE-L scores.</li>
              <li>LLaMA-GPT4 and GPT-4 gradually perform better than Alpaca when the ground truth response length increases.</li>
              <li>LLaMA-GPT4 can closely follow the behavior of GPT-4 across different subsets.</li>
              <li>LLaMA-GPT4 and GPT-4 tend to generate responses that contain simple ground truth answers but add extra words to make the response more chat-like, which may lead to lower ROUGE-L scores.</li>
              </ul>

            <div style="text-align: center;">
              <img src="./images/un_natural_instruction_evaluations.png" alt="" width="60%">
            </div>

            <p style="color:gray; text-align: center;">
              ROUGE-L on Unnatural Instructions.
             </p>


          </div>
          <!-- <div class="tab-pane" id="settings" role="tabpanel">..4.</div> -->
        </div>
        
      

    <h3 class=".display-3">Citation</h3>
    If the paper inspires you and the data is used in your research, please cite us:
        <pre class="citation">@article{peng2023gpt4llm,
          title={Instruction Tuning with GPT-4},
          author={Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao},
          journal={arXiv preprint arXiv:},
          year={2023}
        }
      </pre>
    </div>


    <div class="col-md-12" style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif">

      <h3 class=".display-3">Release and License</h3>
      The data is intended solely for research and non-commercial purposes. Its use is subject to the Terms of Use for data generated by OpenAI. If you discover any potential violations, please contact us. Additionally, the code is governed by the Apache License 2.0.

<br>

<br>

    </div>

    <div class="col-md-12" style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif">

      <h3 class=".display-3">The Team </h3>
      Baolin Peng<sup>*</sup>, Chunyuan Li<sup>*</sup>, Pengcheng He<sup>*</sup>, Michel Galley, Jianfeng Gao <br>
      <sup>*</sup> Equal contribution
    </div>

    <div class="col-md-12" style="font-family: 'Gill Sans', 'Gill Sans MT', Calibri, 'Trebuchet MS', sans-serif">
      <br>
      <h3 class=".display-3">Acknowledgement </h3>
      We thank Guoyin Wang, Haotian Liu and Hao Cheng for valuable discussions and insightful experience sharing on instruction-tuning language models.
We thank the LLaMA team for giving us access to their models.
      <br>
      <br>
    </div>

    </div>
    
    </main>

    <script>


</script>

  </body>
</html>
